#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»“æœæ–‡ä»¶åˆ†æè„šæœ¬
åˆ†æ raw_detections.json, dual_thread_results.json, median_coordinates.json
"""

import json
import os
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime

def load_json_file(filename):
    """åŠ è½½JSONæ–‡ä»¶"""
    if not os.path.exists(filename):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
        return None
    
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"âœ… æˆåŠŸåŠ è½½: {filename}")
        return data
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥ {filename}: {e}")
        return None

def analyze_raw_detections(data):
    """åˆ†æåŸå§‹æ£€æµ‹æ•°æ®"""
    print("\n" + "="*60)
    print("ğŸ“Š åŸå§‹æ£€æµ‹æ•°æ®åˆ†æ (raw_detections.json)")
    print("="*60)
    
    if not data:
        return
    
    # åŸºæœ¬ç»Ÿè®¡
    total_detections = len(data)
    print(f"ğŸ¯ æ£€æµ‹æ€»æ•°: {total_detections}")
    
    # æŒ‰å¸§ç»Ÿè®¡
    frame_counts = Counter(item['frame_id'] for item in data)
    max_frame = max(frame_counts.keys())
    min_frame = min(frame_counts.keys())
    print(f"ğŸ“¹ å¤„ç†å¸§æ•°: {min_frame} - {max_frame} (å…± {max_frame - min_frame + 1} å¸§)")
    print(f"ğŸ“ˆ å¹³å‡æ¯å¸§æ£€æµ‹æ•°: {total_detections / len(frame_counts):.2f}")
    
    # ç½®ä¿¡åº¦ç»Ÿè®¡
    confidences = [item['confidence'] for item in data]
    print(f"ğŸ¯ ç½®ä¿¡åº¦ç»Ÿè®¡:")
    print(f"   æœ€é«˜: {max(confidences):.4f}")
    print(f"   æœ€ä½: {min(confidences):.4f}")
    print(f"   å¹³å‡: {np.mean(confidences):.4f}")
    print(f"   ä¸­ä½æ•°: {np.median(confidences):.4f}")
    
    # æ—¶é—´ç»Ÿè®¡
    timestamps = [item['timestamp'] for item in data]
    duration = max(timestamps) - min(timestamps)
    print(f"â±ï¸ å¤„ç†æ—¶é•¿: {duration:.2f} ç§’")
    print(f"ğŸ”„ æ£€æµ‹é¢‘ç‡: {total_detections / duration:.2f} æ¬¡/ç§’")
    
    # æ£€æµ‹æ¡†å¤§å°ç»Ÿè®¡
    box_areas = []
    for item in data:
        box = item['detection_box']
        width = box[2] - box[0]
        height = box[3] - box[1]
        area = width * height
        box_areas.append(area)
    
    print(f"ğŸ“¦ æ£€æµ‹æ¡†é¢ç§¯ç»Ÿè®¡:")
    print(f"   æœ€å¤§: {max(box_areas)} åƒç´ Â²")
    print(f"   æœ€å°: {min(box_areas)} åƒç´ Â²")
    print(f"   å¹³å‡: {np.mean(box_areas):.0f} åƒç´ Â²")
    
    return {
        'total_detections': total_detections,
        'frame_count': len(frame_counts),
        'duration': duration,
        'avg_confidence': np.mean(confidences)
    }

def analyze_dual_thread_results(data):
    """åˆ†æå‰¯çº¿ç¨‹å¤„ç†ç»“æœ"""
    print("\n" + "="*60)
    print("ğŸ”„ å‰¯çº¿ç¨‹å¤„ç†ç»“æœåˆ†æ (dual_thread_results.json)")
    print("="*60)
    
    if not data:
        return
    
    total_processed = len(data)
    print(f"âš™ï¸ å‰¯çº¿ç¨‹å¤„ç†æ€»æ•°: {total_processed}")
    
    # OCRè¯†åˆ«ç»Ÿè®¡
    ocr_results = [item['detected_number'] for item in data]
    recognized_count = sum(1 for result in ocr_results if result != "æœªè¯†åˆ«")
    unrecognized_count = total_processed - recognized_count
    
    print(f"ğŸ” OCRè¯†åˆ«ç»Ÿè®¡:")
    print(f"   æˆåŠŸè¯†åˆ«: {recognized_count} ({recognized_count/total_processed*100:.1f}%)")
    print(f"   æœªè¯†åˆ«: {unrecognized_count} ({unrecognized_count/total_processed*100:.1f}%)")
    
    # è¯†åˆ«çš„æ•°å­—ç»Ÿè®¡
    if recognized_count > 0:
        recognized_numbers = [result for result in ocr_results if result != "æœªè¯†åˆ«"]
        number_counts = Counter(recognized_numbers)
        print(f"ğŸ“Š è¯†åˆ«åˆ°çš„æ•°å­—åˆ†å¸ƒ:")
        for number, count in sorted(number_counts.items()):
            print(f"   æ•°å­— '{number}': {count} æ¬¡")
    
    # GPSåæ ‡èŒƒå›´
    latitudes = [item['gps_position']['latitude'] for item in data]
    longitudes = [item['gps_position']['longitude'] for item in data]
    
    print(f"ğŸ—ºï¸ GPSåæ ‡èŒƒå›´:")
    print(f"   çº¬åº¦: {min(latitudes):.6f} ~ {max(latitudes):.6f}")
    print(f"   ç»åº¦: {min(longitudes):.6f} ~ {max(longitudes):.6f}")
    print(f"   çº¬åº¦è·¨åº¦: {max(latitudes) - min(latitudes):.6f}Â°")
    print(f"   ç»åº¦è·¨åº¦: {max(longitudes) - min(longitudes):.6f}Â°")
    
    # ç½®ä¿¡åº¦ç»Ÿè®¡
    confidences = [item['confidence'] for item in data]
    print(f"ğŸ¯ å¤„ç†ç›®æ ‡ç½®ä¿¡åº¦:")
    print(f"   å¹³å‡: {np.mean(confidences):.4f}")
    print(f"   æœ€é«˜: {max(confidences):.4f}")
    print(f"   æœ€ä½: {min(confidences):.4f}")
    
    return {
        'total_processed': total_processed,
        'recognized_count': recognized_count,
        'recognition_rate': recognized_count/total_processed*100,
        'coordinate_range': {
            'lat_min': min(latitudes),
            'lat_max': max(latitudes),
            'lon_min': min(longitudes),
            'lon_max': max(longitudes)
        }
    }

def analyze_median_coordinates(data):
    """åˆ†æä¸­ä½æ•°åæ ‡"""
    print("\n" + "="*60)
    print("ğŸ“ ä¸­ä½æ•°åæ ‡åˆ†æ (median_coordinates.json)")
    print("="*60)
    
    if not data:
        return
    
    print(f"ğŸ¯ ç›®æ ‡æ€»æ•°: {data['total_targets']}")
    print(f"ğŸ“ ä¸­ä½æ•°åæ ‡:")
    print(f"   çº¬åº¦: {data['median_latitude']:.6f}Â°")
    print(f"   ç»åº¦: {data['median_longitude']:.6f}Â°")
    
    # è½¬æ¢æ—¶é—´æˆ³
    calc_time = datetime.fromtimestamp(data['calculation_time'])
    print(f"â±ï¸ è®¡ç®—æ—¶é—´: {calc_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    return data

def generate_summary_report(raw_stats, dual_stats, median_data):
    """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“‹ ç»¼åˆåˆ†ææŠ¥å‘Š")
    print("="*60)
    
    if raw_stats and dual_stats:
        processing_rate = (dual_stats['total_processed'] / raw_stats['total_detections']) * 100
        print(f"ğŸ”„ å¤„ç†æ•ˆç‡:")
        print(f"   åŸå§‹æ£€æµ‹: {raw_stats['total_detections']} ä¸ªç›®æ ‡")
        print(f"   å‰¯çº¿ç¨‹å¤„ç†: {dual_stats['total_processed']} ä¸ªç›®æ ‡")
        print(f"   å¤„ç†ç‡: {processing_rate:.1f}%")
        
        if dual_stats['recognized_count'] > 0:
            overall_recognition_rate = (dual_stats['recognized_count'] / raw_stats['total_detections']) * 100
            print(f"   æ•´ä½“OCRæˆåŠŸç‡: {overall_recognition_rate:.1f}%")
    
    print(f"\nâ±ï¸ ç³»ç»Ÿæ€§èƒ½:")
    if raw_stats:
        print(f"   æ£€æµ‹é¢‘ç‡: {raw_stats['total_detections'] / raw_stats['duration']:.1f} æ¬¡/ç§’")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {raw_stats['avg_confidence']:.3f}")
    
    print(f"\nğŸ¯ è´¨é‡è¯„ä¼°:")
    if dual_stats:
        if dual_stats['recognition_rate'] >= 50:
            quality = "ä¼˜ç§€"
        elif dual_stats['recognition_rate'] >= 30:
            quality = "è‰¯å¥½"
        elif dual_stats['recognition_rate'] >= 10:
            quality = "ä¸€èˆ¬"
        else:
            quality = "éœ€æ”¹è¿›"
        print(f"   OCRè¯†åˆ«ç‡: {dual_stats['recognition_rate']:.1f}% ({quality})")

def create_visualization(raw_data, dual_data):
    """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
    print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    try:
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('åŒçº¿ç¨‹SITLç³»ç»Ÿç»“æœåˆ†æ', fontsize=16, fontweight='bold')
        
        # 1. æ¯å¸§æ£€æµ‹æ•°é‡
        if raw_data:
            frame_counts = Counter(item['frame_id'] for item in raw_data)
            frames = sorted(frame_counts.keys())
            counts = [frame_counts[f] for f in frames]
            
            axes[0,0].plot(frames, counts, 'b-', marker='o', markersize=4)
            axes[0,0].set_title('æ¯å¸§æ£€æµ‹æ•°é‡')
            axes[0,0].set_xlabel('å¸§å·')
            axes[0,0].set_ylabel('æ£€æµ‹æ•°é‡')
            axes[0,0].grid(True, alpha=0.3)
        
        # 2. ç½®ä¿¡åº¦åˆ†å¸ƒ
        if raw_data:
            confidences = [item['confidence'] for item in raw_data]
            axes[0,1].hist(confidences, bins=20, alpha=0.7, color='green', edgecolor='black')
            axes[0,1].set_title('æ£€æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ')
            axes[0,1].set_xlabel('ç½®ä¿¡åº¦')
            axes[0,1].set_ylabel('é¢‘æ¬¡')
            axes[0,1].grid(True, alpha=0.3)
        
        # 3. OCRè¯†åˆ«ç»“æœ
        if dual_data:
            ocr_results = [item['detected_number'] for item in dual_data]
            recognized = sum(1 for r in ocr_results if r != "æœªè¯†åˆ«")
            unrecognized = len(ocr_results) - recognized
            
            labels = ['å·²è¯†åˆ«', 'æœªè¯†åˆ«']
            sizes = [recognized, unrecognized]
            colors = ['#66b3ff', '#ff9999']
            
            axes[1,0].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
            axes[1,0].set_title('OCRè¯†åˆ«æˆåŠŸç‡')
        
        # 4. GPSåæ ‡åˆ†å¸ƒ
        if dual_data:
            lats = [item['gps_position']['latitude'] for item in dual_data]
            lons = [item['gps_position']['longitude'] for item in dual_data]
            
            axes[1,1].scatter(lons, lats, alpha=0.6, s=30, c='red')
            axes[1,1].set_title('ç›®æ ‡GPSåæ ‡åˆ†å¸ƒ')
            axes[1,1].set_xlabel('ç»åº¦')
            axes[1,1].set_ylabel('çº¬åº¦')
            axes[1,1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('analysis_results.png', dpi=300, bbox_inches='tight')
        print("âœ… å›¾è¡¨å·²ä¿å­˜ä¸º: analysis_results.png")
        
    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” åŒçº¿ç¨‹SITLç³»ç»Ÿç»“æœåˆ†æ")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®æ–‡ä»¶
    raw_data = load_json_file('raw_detections.json')
    dual_data = load_json_file('dual_thread_results.json')
    median_data = load_json_file('median_coordinates.json')
    
    # åˆ†æå„ä¸ªæ–‡ä»¶
    raw_stats = analyze_raw_detections(raw_data)
    dual_stats = analyze_dual_thread_results(dual_data)
    median_stats = analyze_median_coordinates(median_data)
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    generate_summary_report(raw_stats, dual_stats, median_data)
    
    # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    create_visualization(raw_data, dual_data)
    
    print("\nâœ… åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main() 