#!/usr/bin/env python3
# inference4_realtime_tensorrt.py
# TensorRTä¼˜åŒ–ç‰ˆæœ¬ - é’ˆå¯¹Jetson Orin Nanoä¼˜åŒ–

import cv2
import numpy as np
import time
import os
import threading
from queue import Queue, Empty
import easyocr
import sys
import asyncio
from concurrent.futures import ThreadPoolExecutor

# æ·»åŠ TensorRTä¼˜åŒ–æ£€æµ‹å™¨è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from yolo_trt_utils_optimized import JetsonOptimizedYOLODetector

# å°è¯•å¯¼å…¥Jetsonç›‘æ§å·¥å…·
try:
    from jtop import jtop
    JTOP_AVAILABLE = True
except ImportError:
    JTOP_AVAILABLE = False
    print("âš ï¸  jtopæœªå®‰è£…ï¼Œæ— æ³•ç›‘æ§JetsonçŠ¶æ€")

class FrameBuffer:
    """ä¼˜åŒ–çš„å¸§ç¼“å†²åŒº"""
    def __init__(self, maxsize=2):  # å‡å°‘ç¼“å†²åŒºå¤§å°ä»¥é™ä½å»¶è¿Ÿ
        self.queue = Queue(maxsize=maxsize)
        self.dropped_frames = 0
        self.lock = threading.Lock()
    
    def put_frame(self, frame, timestamp):
        """æ·»åŠ å¸§åˆ°ç¼“å†²åŒº"""
        with self.lock:
            try:
                # å¦‚æœé˜Ÿåˆ—æ»¡äº†ï¼Œå…ˆæ¸…ç©ºå†æ·»åŠ æ–°å¸§
                while self.queue.full():
                    try:
                        self.queue.get_nowait()
                        self.dropped_frames += 1
                    except Empty:
                        break
                
                self.queue.put((frame, timestamp), block=False)
            except:
                self.dropped_frames += 1
    
    def get_frame(self, timeout=0.005):  # å‡å°‘ç­‰å¾…æ—¶é—´
        """è·å–æœ€æ–°å¸§"""
        try:
            return self.queue.get(timeout=timeout)
        except Empty:
            return None, None
    
    def get_dropped_count(self):
        """è·å–ä¸¢å¸§æ•°é‡"""
        with self.lock:
            count = self.dropped_frames
            self.dropped_frames = 0
            return count

class VideoCapture:
    """ä¼˜åŒ–çš„è§†é¢‘æ•è·ç±»"""
    def __init__(self, source):
        self.cap = cv2.VideoCapture(source)
        
        # ä¼˜åŒ–æ‘„åƒå¤´è®¾ç½®
        if isinstance(source, int):
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # å‡å°‘ç¼“å†²åŒº
            self.cap.set(cv2.CAP_PROP_FPS, 30)
            # è®¾ç½®è¾ƒä½åˆ†è¾¨ç‡ä»¥æé«˜æ€§èƒ½
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        
        self.frame_buffer = FrameBuffer(maxsize=2)
        self.fps = self.cap.get(cv2.CAP_PROP_FPS) or 30.0
        self.frame_interval = 1.0 / self.fps
        self.capture_thread = None
        self.running = False
        self.total_frames = 0
        self.start_time = None
        
        # è§†é¢‘æ–‡ä»¶å¤„ç†
        if isinstance(source, str) and source.endswith(('.mp4', '.avi', '.mov', '.mkv')):
            self.total_video_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
            self.is_video_file = True
        else:
            self.total_video_frames = 0
            self.is_video_file = False
    
    def start(self):
        """å¼€å§‹æ•è·"""
        if self.cap.isOpened():
            self.running = True
            self.start_time = time.time()
            self.capture_thread = threading.Thread(target=self._capture_frames)
            self.capture_thread.daemon = True
            self.capture_thread.start()
            return True
        return False
    
    def _capture_frames(self):
        """ä¼˜åŒ–çš„å¸§æ•è·çº¿ç¨‹"""
        last_frame_time = time.time()
        frame_skip_count = 0
        
        while self.running:
            ret, frame = self.cap.read()
            if not ret:
                if self.is_video_file:
                    self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                    continue
                else:
                    break
            
            current_time = time.time()
            self.total_frames += 1
            
            # åŠ¨æ€è·³å¸§ç­–ç•¥
            if self.is_video_file:
                elapsed_time = current_time - self.start_time
                expected_frame = int(elapsed_time * self.fps)
                
                if self.total_frames < expected_frame:
                    skip_frames = min(expected_frame - self.total_frames, 5)
                    for _ in range(skip_frames):
                        ret, frame = self.cap.read()
                        if not ret:
                            break
                        self.total_frames += 1
                        frame_skip_count += 1
            
            # æ·»åŠ å¸§åˆ°ç¼“å†²åŒº
            self.frame_buffer.put_frame(frame, current_time)
            
            # æ§åˆ¶æ•è·é¢‘ç‡
            if not self.is_video_file:
                sleep_time = max(0, self.frame_interval - (current_time - last_frame_time))
                if sleep_time > 0:
                    time.sleep(sleep_time)
            
            last_frame_time = current_time
    
    def read(self):
        """è¯»å–æœ€æ–°å¸§"""
        return self.frame_buffer.get_frame()
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        dropped = self.frame_buffer.get_dropped_count()
        if self.start_time:
            elapsed = time.time() - self.start_time
            actual_fps = self.total_frames / elapsed if elapsed > 0 else 0
        else:
            actual_fps = 0
        
        return {
            'dropped_frames': dropped,
            'total_frames': self.total_frames,
            'actual_fps': actual_fps,
            'target_fps': self.fps
        }
    
    def stop(self):
        """åœæ­¢æ•è·"""
        self.running = False
        if self.capture_thread:
            self.capture_thread.join(timeout=1.0)
        if self.cap:
            self.cap.release()

class AsyncOCRProcessor:
    """å¼‚æ­¥OCRå¤„ç†å™¨"""
    def __init__(self, max_workers=2):
        print("ğŸ”¤ åˆå§‹åŒ–å¼‚æ­¥OCRå¤„ç†å™¨...")
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.reader = easyocr.Reader(['en'], gpu=False, download_enabled=True)
        self.ocr_cache = {}  # ç®€å•çš„OCRç»“æœç¼“å­˜
        self.cache_timeout = 2.0  # ç¼“å­˜è¶…æ—¶æ—¶é—´
        
    def _ocr_recognize(self, image_key, image):
        """æ‰§è¡ŒOCRè¯†åˆ«"""
        # æ£€æŸ¥ç¼“å­˜
        current_time = time.time()
        if image_key in self.ocr_cache:
            result, timestamp = self.ocr_cache[image_key]
            if current_time - timestamp < self.cache_timeout:
                return result
        
        # é¢„å¤„ç†å¢å¼ºå¯¹æ¯”åº¦
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        enhanced = clahe.apply(gray)
        
        # æ‰§è¡ŒOCR
        try:
            results = self.reader.readtext(enhanced, detail=0)
            text = " ".join(results).upper()
            
            # æ›´æ–°ç¼“å­˜
            self.ocr_cache[image_key] = (text, current_time)
            
            # æ¸…ç†è¿‡æœŸç¼“å­˜
            if len(self.ocr_cache) > 20:
                self._cleanup_cache()
            
            return text
        except Exception as e:
            print(f"OCRé”™è¯¯: {e}")
            return ""
    
    def _cleanup_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        current_time = time.time()
        expired_keys = [
            key for key, (_, timestamp) in self.ocr_cache.items()
            if current_time - timestamp > self.cache_timeout
        ]
        for key in expired_keys:
            del self.ocr_cache[key]
    
    def process_async(self, image_key, image):
        """å¼‚æ­¥å¤„ç†OCR"""
        return self.executor.submit(self._ocr_recognize, image_key, image)
    
    def shutdown(self):
        """å…³é—­å¤„ç†å™¨"""
        self.executor.shutdown(wait=True)

class ArrowProcessor:
    """ä¼˜åŒ–çš„ç®­å¤´å¤„ç†å™¨"""
    def __init__(self):
        # åˆå§‹åŒ–å¼‚æ­¥OCR
        self.ocr_processor = AsyncOCRProcessor(max_workers=1)  # Jetsonä¸Šä½¿ç”¨å•çº¿ç¨‹OCR
        
        # çº¢è‰²é˜ˆå€¼èŒƒå›´ï¼ˆHSVé¢œè‰²ç©ºé—´ï¼‰
        self.lower_red1 = np.array([0, 30, 30])
        self.lower_red2 = np.array([150, 30, 30])
        self.upper_red1 = np.array([30, 255, 255])
        self.upper_red2 = np.array([179, 255, 255])
        
        # ä¼˜åŒ–çš„å½¢æ€å­¦å¤„ç†æ ¸
        self.kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        
        # OCRä»»åŠ¡ç®¡ç†
        self.ocr_futures = {}
        self.ocr_results = {}

    def _preprocess_red_mask(self, image):
        """ä¼˜åŒ–çš„çº¢è‰²åŒºåŸŸé¢„å¤„ç†"""
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        mask1 = cv2.inRange(hsv, self.lower_red1, self.upper_red1)
        mask2 = cv2.inRange(hsv, self.lower_red2, self.upper_red2)
        combined = cv2.bitwise_or(mask1, mask2)
        
        # å‡å°‘å½¢æ€å­¦æ“ä½œä»¥æé«˜é€Ÿåº¦
        cleaned = cv2.morphologyEx(combined, cv2.MORPH_OPEN, self.kernel, iterations=1)
        cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_CLOSE, self.kernel, iterations=1)
        return cleaned

    def _correct_rotation(self, image, angle):
        """ä¼˜åŒ–çš„æ—‹è½¬æ ¡æ­£"""
        (h, w) = image.shape[:2]
        center = (w // 2, h // 2)
        
        # æ‰§è¡Œæ—‹è½¬
        M = cv2.getRotationMatrix2D(center, angle, 1.0)
        rotated = cv2.warpAffine(image, M, (w, h), borderValue=(255, 255, 255))
        
        # ç®€åŒ–çš„æ–¹å‘éªŒè¯
        rotated_mask = self._preprocess_red_mask(rotated)
        top = rotated_mask[:h//2, :]
        bottom = rotated_mask[h//2:, :]
        if cv2.countNonZero(bottom) > cv2.countNonZero(top):
            rotated = cv2.rotate(rotated, cv2.ROTATE_180)
        return rotated

    def rotate_arrow(self, crop_image):
        """ä¼˜åŒ–çš„æ—‹è½¬æ ¡æ­£æµç¨‹"""
        # æ£€æŸ¥å›¾åƒå¤§å°ï¼Œå¦‚æœå¤ªå°å°±è·³è¿‡å¤„ç†
        if crop_image.shape[0] < 50 or crop_image.shape[1] < 50:
            return crop_image
            
        mask = self._preprocess_red_mask(crop_image)
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if not contours:
            return crop_image
            
        max_contour = max(contours, key=cv2.contourArea)
        rect = cv2.minAreaRect(max_contour)
        (_, _), (w, h), angle = rect
        
        if w > h:
            angle += 90
        return self._correct_rotation(crop_image, angle)

    def process_detection_async(self, detection_id, crop_image):
        """å¼‚æ­¥å¤„ç†æ£€æµ‹ç»“æœ"""
        # æ—‹è½¬æ ¡æ­£
        rotated = self.rotate_arrow(crop_image)
        
        # æäº¤OCRä»»åŠ¡
        future = self.ocr_processor.process_async(detection_id, rotated)
        self.ocr_futures[detection_id] = future
        
        return rotated

    def get_ocr_result(self, detection_id):
        """è·å–OCRç»“æœ"""
        if detection_id in self.ocr_futures:
            future = self.ocr_futures[detection_id]
            if future.done():
                try:
                    result = future.result()
                    self.ocr_results[detection_id] = result
                    del self.ocr_futures[detection_id]
                    return result
                except Exception as e:
                    print(f"OCRä»»åŠ¡å¤±è´¥: {e}")
                    del self.ocr_futures[detection_id]
                    return ""
        
        return self.ocr_results.get(detection_id, "å¤„ç†ä¸­...")

    def cleanup_old_results(self, active_detection_ids):
        """æ¸…ç†ä¸å†éœ€è¦çš„OCRç»“æœ"""
        # æ¸…ç†å®Œæˆçš„ç»“æœ
        for detection_id in list(self.ocr_results.keys()):
            if detection_id not in active_detection_ids:
                del self.ocr_results[detection_id]
        
        # æ¸…ç†æœªå®Œæˆçš„ä»»åŠ¡
        for detection_id in list(self.ocr_futures.keys()):
            if detection_id not in active_detection_ids:
                self.ocr_futures[detection_id].cancel()
                del self.ocr_futures[detection_id]

    def shutdown(self):
        """å…³é—­å¤„ç†å™¨"""
        self.ocr_processor.shutdown()

class JetsonMonitor:
    """Jetsonæ€§èƒ½ç›‘æ§å™¨"""
    def __init__(self):
        self.jetson_available = JTOP_AVAILABLE
        self.stats = {}
        
    def get_stats(self):
        """è·å–JetsonçŠ¶æ€"""
        if not self.jetson_available:
            return {}
        
        try:
            with jtop() as jetson:
                if jetson.ok():
                    self.stats = {
                        'gpu_usage': jetson.gpu.get('GR3D', {}).get('val', 0),
                        'cpu_usage': sum(jetson.cpu.values()) / len(jetson.cpu),
                        'memory_used': jetson.memory['RAM']['used'],
                        'memory_total': jetson.memory['RAM']['tot'],
                        'temperature': jetson.temperature.get('CPU', 0),
                        'power': jetson.power.get('cur', 0),
                        'nvpmodel': jetson.nvpmodel.get('name', 'Unknown')
                    }
        except Exception as e:
            print(f"Jetsonç›‘æ§é”™è¯¯: {e}")
        
        return self.stats

def main():
    # è·å–æ¨¡å‹è·¯å¾„
    possible_model_dirs = [
        "../weights",
        "weights", 
        "../ready/weights",
        "D:/AirmodelingTeam/CQU_Ground_Recog_Strile_YoloOcr/weights"
    ]
    
    model_dir = None
    for path in possible_model_dirs:
        if os.path.exists(path):
            model_dir = path
            break
    
    if model_dir is None:
        print("âŒ æœªæ‰¾åˆ°æ¨¡å‹ç›®å½•ï¼Œè¯·æ£€æŸ¥weightsæ–‡ä»¶å¤¹ä½ç½®")
        return
    
    # ä¼˜å…ˆä½¿ç”¨TensorRTå¼•æ“
    trt_model_path = os.path.join(model_dir, "best1.engine")
    pt_model_path = os.path.join(model_dir, "best1.pt")
    
    if os.path.exists(trt_model_path):
        model_path = trt_model_path
        print(f"ğŸš€ ä½¿ç”¨TensorRTå¼•æ“: {trt_model_path}")
    elif os.path.exists(pt_model_path):
        model_path = pt_model_path
        print(f"ğŸ“¦ ä½¿ç”¨PyTorchæ¨¡å‹: {pt_model_path}")
    else:
        print("âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        return

    # åˆå§‹åŒ–ç»„ä»¶
    try:
        print("ğŸ”§ åˆå§‹åŒ–TensorRTä¼˜åŒ–æ£€æµ‹å™¨...")
        detector = JetsonOptimizedYOLODetector(model_path=model_path, conf_thres=0.25)
        
        print("ğŸ”„ åˆå§‹åŒ–ç®­å¤´å¤„ç†å™¨...")
        processor = ArrowProcessor()
        
        print("ğŸ“Š åˆå§‹åŒ–Jetsonç›‘æ§å™¨...")
        jetson_monitor = JetsonMonitor()
        
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # è§†é¢‘æºè®¾ç½®
    video_sources = [
        0,  # é»˜è®¤æ‘„åƒå¤´
        1,  # ç¬¬äºŒä¸ªæ‘„åƒå¤´
        "test_video.mp4",  # æµ‹è¯•è§†é¢‘æ–‡ä»¶
    ]
    
    # å°è¯•æ‰“å¼€è§†é¢‘æº
    video_capture = None
    for source in video_sources:
        try:
            temp_cap = VideoCapture(source)
            if temp_cap.start():
                video_capture = temp_cap
                print(f"ğŸ“¹ æˆåŠŸæ‰“å¼€è§†é¢‘æº: {source}")
                break
            else:
                temp_cap.stop()
        except Exception as e:
            print(f"âš ï¸  å°è¯•æ‰“å¼€è§†é¢‘æº {source} å¤±è´¥: {e}")
    
    if video_capture is None:
        print("âŒ æ— æ³•æ‰“å¼€ä»»ä½•è§†é¢‘æº")
        return
    
    # æ˜¾ç¤ºè®¾ç½®
    cv2.namedWindow("TensorRTå®æ—¶æ£€æµ‹", cv2.WINDOW_NORMAL)
    cv2.resizeWindow("TensorRTå®æ—¶æ£€æµ‹", 1280, 720)
    
    # æ€§èƒ½ç»Ÿè®¡
    process_times = []
    last_stats_time = time.time()
    processed_frames = 0
    detection_id_counter = 0
    
    print("ğŸ¯ TensorRTä¼˜åŒ–ç‰ˆæœ¬å¯åŠ¨å®Œæˆ!")
    print("æŒ‰'q'é”®é€€å‡ºï¼ŒæŒ‰'s'é”®æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ï¼ŒæŒ‰'j'é”®æ˜¾ç¤ºJetsonçŠ¶æ€")

    try:
        while True:
            # è·å–æœ€æ–°å¸§
            frame, timestamp = video_capture.read()
            if frame is None:
                time.sleep(0.005)  # å‡å°‘ç­‰å¾…æ—¶é—´
                continue
            
            process_start = time.time()
            
            # åŠ¨æ€è°ƒæ•´å›¾åƒå¤§å°ï¼ˆæ ¹æ®æ€§èƒ½è°ƒæ•´ï¼‰
            avg_process_time = sum(process_times[-10:]) / len(process_times[-10:]) if process_times else 0
            if avg_process_time > 0.1:  # å¦‚æœå¤„ç†æ—¶é—´è¶…è¿‡100ms
                scale_percent = 60  # é™ä½åˆ†è¾¨ç‡
            else:
                scale_percent = 75  # æ­£å¸¸åˆ†è¾¨ç‡
            
            width = int(frame.shape[1] * scale_percent / 100)
            height = int(frame.shape[0] * scale_percent / 100)
            frame = cv2.resize(frame, (width, height))

            # YOLOæ£€æµ‹
            detections = detector.detect(frame)
            
            # å¤„ç†æ£€æµ‹ç»“æœ
            preview_height = 100
            preview_width = 100
            spacing = 10
            active_detection_ids = []
            
            for i, det in enumerate(detections):
                x1, y1, x2, y2 = map(int, det['box'])
                detection_id = f"det_{detection_id_counter}_{i}"
                active_detection_ids.append(detection_id)
                
                try:
                    # æ‰©å±•æ£€æµ‹æ¡†ï¼ˆå‡å°‘æ‰©å±•æ¯”ä¾‹ä»¥æé«˜æ€§èƒ½ï¼‰
                    expand_ratio = 0.05
                    width_det = x2 - x1
                    height_det = y2 - y1
                    expand_w = int(width_det * expand_ratio)
                    expand_h = int(height_det * expand_ratio)
                    
                    x1_exp = max(0, x1 - expand_w)
                    y1_exp = max(0, y1 - expand_h)
                    x2_exp = min(frame.shape[1], x2 + expand_w)
                    y2_exp = min(frame.shape[0], y2 + expand_h)
                    
                    # è£å‰ªåŒºåŸŸ
                    crop = frame[y1_exp:y2_exp, x1_exp:x2_exp].copy()
                    if crop.size == 0:
                        continue
                    
                    # å¼‚æ­¥å¤„ç†æ—‹è½¬å’ŒOCR
                    rotated = processor.process_detection_async(detection_id, crop)
                    
                    # è·å–OCRç»“æœï¼ˆå¯èƒ½æ˜¯ç¼“å­˜çš„æˆ–æ­£åœ¨å¤„ç†çš„ï¼‰
                    text = processor.get_ocr_result(detection_id)
                    
                    # å¯è§†åŒ–
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    cv2.putText(frame, f"{text} ({det['confidence']:.2f})", 
                               (x1, y2 + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
                    
                    # æ˜¾ç¤ºé¢„è§ˆï¼ˆå‡å°‘é¢„è§ˆå¤§å°ï¼‰
                    if i < 5:  # æœ€å¤šæ˜¾ç¤º5ä¸ªé¢„è§ˆ
                        preview = cv2.resize(rotated, (preview_width, preview_height))
                        x_offset = 10 + i * (preview_width + spacing)
                        y_offset = 10
                        if x_offset + preview_width <= frame.shape[1] and y_offset + preview_height <= frame.shape[0]:
                            frame[y_offset:y_offset + preview_height, x_offset:x_offset + preview_width] = preview
                    
                except Exception as e:
                    continue
            
            # æ¸…ç†æ—§çš„OCRç»“æœ
            processor.cleanup_old_results(active_detection_ids)
            detection_id_counter += 1
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            process_time = time.time() - process_start
            process_times.append(process_time)
            if len(process_times) > 30:
                process_times.pop(0)
            
            processed_frames += 1
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = video_capture.get_stats()
            detector_stats = detector.get_performance_stats()
            jetson_stats = jetson_monitor.get_stats()
            
            # æ˜¾ç¤ºæ€§èƒ½ä¿¡æ¯
            current_time = time.time()
            if current_time - last_stats_time >= 1.0:
                processing_fps = processed_frames / (current_time - last_stats_time)
                processed_frames = 0
                last_stats_time = current_time
            else:
                processing_fps = 0
            
            avg_process_time = sum(process_times) / len(process_times) if process_times else 0
            
            # æ˜¾ç¤ºä¿¡æ¯
            info_texts = [
                f"ğŸš€ TensorRT: {detector.using_trt}",
                f"ğŸ“Š å¤„ç†FPS: {processing_fps:.1f}",
                f"ğŸ¥ è§†é¢‘FPS: {stats['actual_fps']:.1f}/{stats['target_fps']:.1f}",
                f"â±ï¸  å¤„ç†æ—¶é—´: {avg_process_time*1000:.1f}ms",
                f"ğŸ“‰ ä¸¢å¸§: {stats['dropped_frames']}",
            ]
            
            # æ·»åŠ JetsonçŠ¶æ€ä¿¡æ¯
            if jetson_stats:
                info_texts.extend([
                    f"ğŸ”¥ GPU: {jetson_stats.get('gpu_usage', 0):.1f}%",
                    f"ğŸŒ¡ï¸  æ¸©åº¦: {jetson_stats.get('temperature', 0):.1f}Â°C",
                    f"âš¡ åŠŸè€—: {jetson_stats.get('power', 0):.1f}W"
                ])
            
            for i, text in enumerate(info_texts):
                cv2.putText(frame, text, (10, 150 + i * 25), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)

            # æ˜¾ç¤ºç»“æœ
            cv2.imshow("TensorRTå®æ—¶æ£€æµ‹", frame)
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('s'):
                print(f"\n=== è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ ===")
                print(f"è§†é¢‘FPS: {stats['actual_fps']:.2f}/{stats['target_fps']:.2f}")
                print(f"å¤„ç†FPS: {processing_fps:.2f}")
                print(f"å¹³å‡å¤„ç†æ—¶é—´: {avg_process_time*1000:.2f}ms")
                print(f"TensorRTçŠ¶æ€: {detector.using_trt}")
                print(f"æ£€æµ‹å™¨ç»Ÿè®¡: {detector_stats}")
            elif key == ord('j') and jetson_stats:
                print(f"\n=== JetsonçŠ¶æ€ ===")
                for key, value in jetson_stats.items():
                    print(f"{key}: {value}")

    except KeyboardInterrupt:
        print("\nâ¹ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·")
    except Exception as e:
        print(f"âŒ è¿è¡Œæ—¶é”™è¯¯: {e}")
    finally:
        # é‡Šæ”¾èµ„æº
        print("ğŸ§¹ æ¸…ç†èµ„æºä¸­...")
        if video_capture:
            video_capture.stop()
        processor.shutdown()
        cv2.destroyAllWindows()
        
        # æœ€ç»ˆç»Ÿè®¡
        final_stats = video_capture.get_stats() if video_capture else {}
        final_detector_stats = detector.get_performance_stats()
        print(f"\n=== æœ€ç»ˆç»Ÿè®¡ ===")
        print(f"æ€»å¤„ç†å¸§æ•°: {final_stats.get('total_frames', 0)}")
        print(f"å¹³å‡è§†é¢‘FPS: {final_stats.get('actual_fps', 0):.2f}")
        print(f"TensorRTå¹³å‡FPS: {final_detector_stats.get('fps', 0):.2f}")
        if process_times:
            print(f"å¹³å‡å¤„ç†æ—¶é—´: {sum(process_times)/len(process_times)*1000:.2f}ms")

if __name__ == "__main__":
    main() 