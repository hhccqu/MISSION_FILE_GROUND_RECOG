#!/usr/bin/env python3
# cuda_diagnosis.py
# CUDAé—®é¢˜è¯Šæ–­è„šæœ¬

import sys
import os
import subprocess

def run_command(cmd):
    """æ‰§è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=10)
        return result.returncode == 0, result.stdout.strip(), result.stderr.strip()
    except:
        return False, "", "å‘½ä»¤æ‰§è¡Œå¤±è´¥"

def check_cuda_system():
    """æ£€æŸ¥ç³»ç»Ÿçº§CUDA"""
    print("ğŸ”§ æ£€æŸ¥ç³»ç»Ÿçº§CUDAå®‰è£…")
    print("-" * 40)
    
    # æ£€æŸ¥nvidia-smi
    success, output, error = run_command("nvidia-smi")
    if success:
        print("âœ… nvidia-smi å¯ç”¨")
        print(f"GPUä¿¡æ¯:\n{output[:200]}...")
    else:
        print("âŒ nvidia-smi ä¸å¯ç”¨")
        print(f"é”™è¯¯: {error}")
    
    # æ£€æŸ¥nvcc
    success, output, error = run_command("nvcc --version")
    if success:
        print("âœ… nvcc (CUDAç¼–è¯‘å™¨) å¯ç”¨")
        for line in output.split('\n'):
            if 'release' in line:
                print(f"CUDAç‰ˆæœ¬: {line}")
    else:
        print("âŒ nvcc ä¸å¯ç”¨")
    
    # æ£€æŸ¥CUDAè·¯å¾„
    cuda_paths = [
        "/usr/local/cuda",
        "/usr/local/cuda-11.4",
        "/usr/local/cuda-11.8",
        "/usr/local/cuda-12.0"
    ]
    
    for path in cuda_paths:
        if os.path.exists(path):
            print(f"âœ… æ‰¾åˆ°CUDAå®‰è£…: {path}")
            break
    else:
        print("âŒ æœªæ‰¾åˆ°CUDAå®‰è£…è·¯å¾„")

def check_pytorch_cuda():
    """æ£€æŸ¥PyTorchçš„CUDAæ”¯æŒ"""
    print("\nğŸ”¥ æ£€æŸ¥PyTorch CUDAæ”¯æŒ")
    print("-" * 40)
    
    try:
        import torch
        print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        # æ£€æŸ¥CUDAç¼–è¯‘æ”¯æŒ
        cuda_available = torch.cuda.is_available()
        print(f"CUDAå¯ç”¨æ€§: {'âœ… å¯ç”¨' if cuda_available else 'âŒ ä¸å¯ç”¨'}")
        
        if cuda_available:
            # GPUä¿¡æ¯
            device_count = torch.cuda.device_count()
            print(f"GPUæ•°é‡: {device_count}")
            
            for i in range(device_count):
                gpu_name = torch.cuda.get_device_name(i)
                print(f"GPU {i}: {gpu_name}")
            
            # CUDAç‰ˆæœ¬
            cuda_version = torch.version.cuda
            print(f"PyTorch CUDAç‰ˆæœ¬: {cuda_version}")
            
            # å†…å­˜ä¿¡æ¯
            memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"GPUå†…å­˜: {memory_total:.1f} GB")
            
            # ç®€å•è®¡ç®—æµ‹è¯•
            try:
                x = torch.randn(1000, 1000)
                x_gpu = x.cuda()
                y = torch.mm(x_gpu, x_gpu)
                print("âœ… CUDAè®¡ç®—æµ‹è¯•: æˆåŠŸ")
            except Exception as e:
                print(f"âŒ CUDAè®¡ç®—æµ‹è¯•: å¤±è´¥ - {e}")
        else:
            # åˆ†æä¸ºä»€ä¹ˆCUDAä¸å¯ç”¨
            print("\nğŸ” åˆ†æCUDAä¸å¯ç”¨çš„åŸå› :")
            
            # æ£€æŸ¥æ˜¯å¦ç¼–è¯‘æ—¶åŒ…å«CUDA
            if hasattr(torch.version, 'cuda') and torch.version.cuda is None:
                print("âŒ PyTorchç¼–è¯‘æ—¶æœªåŒ…å«CUDAæ”¯æŒ")
                print("   è§£å†³æ–¹æ¡ˆ: å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch")
            else:
                print("âœ… PyTorchç¼–è¯‘æ—¶åŒ…å«CUDAæ”¯æŒ")
            
            # æ£€æŸ¥CUDAé©±åŠ¨
            success, _, _ = run_command("nvidia-smi")
            if not success:
                print("âŒ NVIDIAé©±åŠ¨æœªå®‰è£…æˆ–ä¸å¯ç”¨")
                print("   è§£å†³æ–¹æ¡ˆ: å®‰è£…NVIDIAé©±åŠ¨")
            else:
                print("âœ… NVIDIAé©±åŠ¨å¯ç”¨")
            
            # æ£€æŸ¥CUDAè¿è¡Œæ—¶
            success, _, _ = run_command("nvcc --version")
            if not success:
                print("âŒ CUDAå·¥å…·åŒ…æœªå®‰è£…")
                print("   è§£å†³æ–¹æ¡ˆ: å®‰è£…CUDA Toolkit")
            else:
                print("âœ… CUDAå·¥å…·åŒ…å·²å®‰è£…")
    
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")

def check_environment_variables():
    """æ£€æŸ¥ç¯å¢ƒå˜é‡"""
    print("\nğŸŒ æ£€æŸ¥CUDAç¯å¢ƒå˜é‡")
    print("-" * 40)
    
    important_vars = [
        "CUDA_HOME",
        "CUDA_ROOT", 
        "PATH",
        "LD_LIBRARY_PATH",
        "CUDA_VISIBLE_DEVICES"
    ]
    
    for var in important_vars:
        value = os.environ.get(var, "æœªè®¾ç½®")
        if var in ["PATH", "LD_LIBRARY_PATH"]:
            if "cuda" in value.lower():
                print(f"âœ… {var}: åŒ…å«CUDAè·¯å¾„")
            else:
                print(f"âš ï¸  {var}: å¯èƒ½ç¼ºå°‘CUDAè·¯å¾„")
        else:
            print(f"{'âœ…' if value != 'æœªè®¾ç½®' else 'âš ï¸ '} {var}: {value}")

def check_jetson_specific():
    """æ£€æŸ¥Jetsonç‰¹å®šé…ç½®"""
    print("\nğŸ¤– æ£€æŸ¥Jetsonç‰¹å®šé…ç½®")
    print("-" * 40)
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºJetsonè®¾å¤‡
    if os.path.exists('/proc/device-tree/model'):
        with open('/proc/device-tree/model', 'r') as f:
            model = f.read().strip()
        if 'Jetson' in model:
            print(f"âœ… Jetsonè®¾å¤‡: {model}")
            
            # æ£€æŸ¥JetPack
            success, output, _ = run_command("dpkg -l | grep nvidia-jetpack")
            if success:
                print("âœ… JetPackå·²å®‰è£…")
            else:
                print("âŒ JetPackæœªå®‰è£…")
            
            # æ£€æŸ¥åŠŸè€—æ¨¡å¼
            success, output, _ = run_command("sudo nvpmodel -q")
            if success:
                print(f"åŠŸè€—æ¨¡å¼ä¿¡æ¯:\n{output}")
            else:
                print("âš ï¸  æ— æ³•æ£€æŸ¥åŠŸè€—æ¨¡å¼")
        else:
            print(f"âŒ éJetsonè®¾å¤‡: {model}")
    else:
        print("âŒ æ— æ³•æ£€æµ‹è®¾å¤‡ç±»å‹")

def provide_solutions():
    """æä¾›è§£å†³æ–¹æ¡ˆ"""
    print("\nğŸ’¡ CUDAé—®é¢˜è§£å†³æ–¹æ¡ˆ")
    print("=" * 50)
    
    print("1. ğŸ”§ å¦‚æœæ˜¯PyTorch CUDAé—®é¢˜:")
    print("   # å¸è½½å½“å‰PyTorch")
    print("   pip uninstall torch torchvision torchaudio")
    print("   # å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch")
    print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    
    print("\n2. ğŸŒ å¦‚æœæ˜¯ç¯å¢ƒå˜é‡é—®é¢˜:")
    print("   export CUDA_HOME=/usr/local/cuda")
    print("   export PATH=$PATH:$CUDA_HOME/bin")
    print("   export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CUDA_HOME/lib64")
    
    print("\n3. ğŸš€ å¦‚æœæ˜¯Jetsonæ€§èƒ½æ¨¡å¼é—®é¢˜:")
    print("   sudo nvpmodel -m 0  # è®¾ç½®æœ€é«˜æ€§èƒ½æ¨¡å¼")
    print("   sudo jetson_clocks   # é”å®šæœ€é«˜é¢‘ç‡")
    
    print("\n4. ğŸ”„ é‡å¯ç›¸å…³æœåŠ¡:")
    print("   # é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡")
    print("   source ~/.bashrc")
    print("   # æˆ–é‡æ–°ç™»å½•")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” CUDAé—®é¢˜è¯Šæ–­å·¥å…·")
    print("=" * 50)
    
    check_cuda_system()
    check_pytorch_cuda()
    check_environment_variables()
    check_jetson_specific()
    provide_solutions()
    
    print("\nğŸ“‹ è¯Šæ–­å®Œæˆï¼")
    print("è¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯è§£å†³CUDAé—®é¢˜ã€‚")

if __name__ == "__main__":
    main() 