# Jetson Orin Nano TensorRT é…ç½®æ•™ç¨‹

## ğŸ“‹ ç›®å½•
1. [ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥](#1-ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥)
2. [JetPackå®‰è£…é…ç½®](#2-jetpackå®‰è£…é…ç½®)
3. [YOLOæ¨¡å‹TensorRTè½¬æ¢](#3-yoloæ¨¡å‹tensorrtè½¬æ¢)
4. [ä»£ç ä¼˜åŒ–](#4-ä»£ç ä¼˜åŒ–)
5. [æ€§èƒ½æµ‹è¯•](#5-æ€§èƒ½æµ‹è¯•)
6. [æ•…éšœæ’é™¤](#6-æ•…éšœæ’é™¤)

---

## 1. ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥

### 1.1 æ£€æŸ¥Jetsonå‹å·å’ŒJetPackç‰ˆæœ¬
```bash
# æ£€æŸ¥è®¾å¤‡å‹å·
cat /proc/device-tree/model

# æ£€æŸ¥JetPackç‰ˆæœ¬
sudo apt show nvidia-jetpack

# æ£€æŸ¥CUDAç‰ˆæœ¬
nvcc --version

# æ£€æŸ¥TensorRTç‰ˆæœ¬
dpkg -l | grep tensorrt
```

### 1.2 ç³»ç»Ÿæ€§èƒ½è®¾ç½®
```bash
# è®¾ç½®æœ€é«˜æ€§èƒ½æ¨¡å¼ (15W)
sudo nvpmodel -m 0

# é”å®šæœ€é«˜é¢‘ç‡
sudo jetson_clocks

# æŸ¥çœ‹å½“å‰æ¨¡å¼
sudo nvpmodel -q --verbose
```

---

## 2. JetPackå®‰è£…é…ç½®

### 2.1 æ›´æ–°JetPack (å¦‚æœéœ€è¦)
```bash
# æ›´æ–°åŒ…åˆ—è¡¨
sudo apt update

# å®‰è£…å®Œæ•´JetPack
sudo apt install nvidia-jetpack

# æˆ–è€…å®‰è£…å¼€å‘ç‰ˆæœ¬
sudo apt install nvidia-jetpack-dev
```

### 2.2 å®‰è£…ç›‘æ§å·¥å…·
```bash
# å®‰è£…jtopç³»ç»Ÿç›‘æ§
sudo pip3 install jetson-stats

# é‡å¯åä½¿ç”¨
sudo reboot
# é‡å¯åè¿è¡Œ
jtop
```

### 2.3 ç¯å¢ƒå˜é‡é…ç½®
```bash
# æ·»åŠ åˆ° ~/.bashrc
echo 'export CUDA_HOME=/usr/local/cuda' >> ~/.bashrc
echo 'export PATH=$PATH:$CUDA_HOME/bin' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CUDA_HOME/lib64' >> ~/.bashrc

# é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡
source ~/.bashrc
```

---

## 3. YOLOæ¨¡å‹TensorRTè½¬æ¢

### 3.1 å®‰è£…å¿…è¦çš„PythonåŒ…
```bash
# å®‰è£…Ultralytics YOLO
pip3 install ultralytics

# å®‰è£…TensorRT Pythonæ¥å£
pip3 install pycuda

# éªŒè¯å®‰è£…
python3 -c "import tensorrt as trt; print(f'TensorRTç‰ˆæœ¬: {trt.__version__}')"
```

### 3.2 è½¬æ¢YOLOæ¨¡å‹ä¸ºTensorRTå¼•æ“

#### æ–¹æ³•1: ä½¿ç”¨Ultralytics (æ¨è)
```bash
# è¿›å…¥weightsç›®å½•
cd weights/

# FP16è½¬æ¢ (æ¨è)
yolo export model=best.pt format=engine half=True device=0

# INT8è½¬æ¢ (æœ€é«˜æ€§èƒ½ï¼Œéœ€è¦æ ¡å‡†æ•°æ®)
yolo export model=best.pt format=engine int8=True data=../datasets/your_dataset.yaml device=0
```

#### æ–¹æ³•2: ä½¿ç”¨trtexecå·¥å…·
```bash
# å…ˆè½¬æ¢ä¸ºONNX
yolo export model=best.pt format=onnx

# ä½¿ç”¨trtexecè½¬æ¢
/usr/src/tensorrt/bin/trtexec \
    --onnx=best.onnx \
    --saveEngine=best_orin_fp16.engine \
    --fp16 \
    --workspace=2048 \
    --minShapes=images:1x3x640x640 \
    --optShapes=images:1x3x640x640 \
    --maxShapes=images:1x3x640x640 \
    --verbose
```

### 3.3 éªŒè¯è½¬æ¢ç»“æœ
```bash
# æ£€æŸ¥ç”Ÿæˆçš„å¼•æ“æ–‡ä»¶
ls -la *.engine

# ä½¿ç”¨trtexecæµ‹è¯•å¼•æ“æ€§èƒ½
/usr/src/tensorrt/bin/trtexec \
    --loadEngine=best.engine \
    --batch=1 \
    --iterations=100 \
    --avgRuns=10
```

---

## 4. ä»£ç ä¼˜åŒ–

å‚è€ƒ `inference4_realtime_tensorrt.py` ä¸­çš„ä¼˜åŒ–å®ç°ï¼š

### 4.1 ä¸»è¦ä¼˜åŒ–ç‚¹
- ä½¿ç”¨TensorRTå¼•æ“è¿›è¡Œæ¨ç†
- ä¼˜åŒ–å†…å­˜ç®¡ç†
- å‡å°‘ä¸å¿…è¦çš„æ•°æ®æ‹·è´
- å¼‚æ­¥å¤„ç†OCR

### 4.2 æ€§èƒ½ç›‘æ§
- é›†æˆjtopç›‘æ§
- å®æ—¶FPSæ˜¾ç¤º
- GPUä½¿ç”¨ç‡ç›‘æ§

---

## 5. æ€§èƒ½æµ‹è¯•

### 5.1 è¿è¡Œä¼˜åŒ–åçš„ä»£ç 
```bash
# ç¡®ä¿åœ¨æ­£ç¡®ç›®å½•
cd LATEST_CODE/

# è¿è¡ŒTensorRTä¼˜åŒ–ç‰ˆæœ¬
python3 inference4_realtime_tensorrt.py
```

### 5.2 æ€§èƒ½å¯¹æ¯”æµ‹è¯•
```bash
# è¿è¡Œæ€§èƒ½æµ‹è¯•è„šæœ¬
python3 ../TensorRT/performance_test.py
```

### 5.3 é¢„æœŸæ€§èƒ½æå‡
- **FP32 â†’ FP16**: 50-80% é€Ÿåº¦æå‡
- **FP32 â†’ INT8**: 100-200% é€Ÿåº¦æå‡
- **å†…å­˜ä½¿ç”¨**: å‡å°‘30-50%

---

## 6. æ•…éšœæ’é™¤

### 6.1 å¸¸è§é—®é¢˜

#### é—®é¢˜1: TensorRTç‰ˆæœ¬ä¸å…¼å®¹
```bash
# è§£å†³æ–¹æ¡ˆ: é‡æ–°å®‰è£…åŒ¹é…ç‰ˆæœ¬
sudo apt remove --purge nvidia-tensorrt
sudo apt install nvidia-tensorrt-dev
```

#### é—®é¢˜2: CUDAå†…å­˜ä¸è¶³
```bash
# è§£å†³æ–¹æ¡ˆ: å¢åŠ swapç©ºé—´
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

#### é—®é¢˜3: æ¨¡å‹è½¬æ¢å¤±è´¥
```bash
# æ£€æŸ¥ONNXæ¨¡å‹
python3 -c "
import onnx
model = onnx.load('best.onnx')
onnx.checker.check_model(model)
print('ONNXæ¨¡å‹éªŒè¯é€šè¿‡')
"
```

### 6.2 è°ƒè¯•å·¥å…·
```bash
# ä½¿ç”¨jtopç›‘æ§ç³»ç»Ÿèµ„æº
jtop

# æ£€æŸ¥CUDAè®¾å¤‡
python3 -c "
import torch
print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')
print(f'CUDAè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}')
print(f'å½“å‰è®¾å¤‡: {torch.cuda.get_device_name(0)}')
"
```

---

## 7. ä¼˜åŒ–å»ºè®®

### 7.1 ç³»ç»Ÿçº§ä¼˜åŒ–
- ä½¿ç”¨é«˜é€ŸSDå¡æˆ–NVMe SSD
- ç¡®ä¿æ•£çƒ­è‰¯å¥½
- å…³é—­ä¸å¿…è¦çš„ç³»ç»ŸæœåŠ¡

### 7.2 ä»£ç çº§ä¼˜åŒ–
- ä½¿ç”¨æ‰¹å¤„ç†æ¨ç†
- å‡å°‘Python-CUDAæ•°æ®ä¼ è¾“
- å¼‚æ­¥å¤„ç†éå…³é”®ä»»åŠ¡

---

## ğŸ“ æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. JetPackç‰ˆæœ¬æ˜¯å¦ä¸º5.1+
2. CUDAå’ŒTensorRTæ˜¯å¦æ­£ç¡®å®‰è£…
3. æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´
4. ç³»ç»Ÿèµ„æºæ˜¯å¦å……è¶³

æ›´å¤šä¿¡æ¯è¯·å‚è€ƒNVIDIA Jetsonå®˜æ–¹æ–‡æ¡£ã€‚ 